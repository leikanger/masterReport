



	\section{General Design of the Simulation Software}

		When designing a simulator of neural networks, one has to consider simulation time explicitly.
		Networks of neurons are highly concurrent, but have to be simulated by a sequential composition in the digital computer. 
		Thus, asynchronism for the nodes has to be emulated in the simulation.
		One way to achieve this is to separate the considered simulation time interval into discrete time slices(``time steps''), and let time in the simulation be expressed by the integer time--step number.
		Before the design of time emulation in $auroSim$ is presented, the concept of concurrent is defined in a way that is valid for discrete time.

		
		\begin{mydef}
			Two tasks occur simultaneously if they can not be separated by their time of occurrence. 
		\end{mydef}

		Two events during the same time step are therefore defined to happen simultaneously, unless additional information about timing is provided.		
		One approach for emulating concurrency is to let discrete time be defined as a discretization of the real world's clock($RWC$).
%		One approach for the emulation of concurrency is to let discrete time be defined as a discretization of the real world's clock(...abbreviation...).
		It is important that the whole list of tasks is completed before time is incremented, so that no tasks are lost or delayed to subsequent time steps. %iteration.
%TODO TODO TODO TODO  Dependence: Find a synonym. Uses it too many times.. (two times on two subsequent sentences) TODO TODO TODO TODO TODO 
%		This creates a strong dependence between the maximal workload of the simulation and the minimal computational time step.
		This creates a strong dependence between the maximal workload and the minimal computational time step in a simulation.
		Such a constraint is wasteful and clearly undesirable.

\begin{figure}[hbt!p]
	\centering
	\includegraphics[width=0.65\textwidth]{timeByAlternatingTaskLists}
	\caption[Time simulation by alternating task lists]{
			Time simulation based on the sequential computation in the digital computer.
			Iteration $t_{n-1}$ have list $A$ as the active list. Two new tasks, $t_{1,1}$ and $t_{1,2}$ is generated by task $t_1$ and inserted into the alternative list(list $B$). Task $t_2$ generates task $t_{2,1}$.
			When all tasks in the active list $A$ is completed, time is iterated and list $B$ is set as the active list. 
			The next computational time step with $B$ as the active list is illustrated in the lower part of the figure.
			Note that no tasks are inserted into the currently active list.
			}
	\label{figTimePropagationByAlternatingTaskLists}
\end{figure}

		An alternative approach is to utilize a scheme based on serial execution.
		If all tasks to be executed simultaneously are located in one of two lists(list A), new tasks induced by these actions can be inserted into the other list(list B).
		When all tasks in one list are completed, the variable that represents time is incremented, and the alternative list becomes the active list.
		Since causality defines that the effect happens \underline{after} its cause, elements can not be inserted into the active list during its execution.
		This serial approach enables concurrency to be simulated without dependence of $RWC$, and one does not have to consider the maximal work load of the simulation. %system.
		The simulation software implemented in this work utilize a modification of this latter time scheme, as presented in section \ref{ssecTime}.



		\subsection{Simulator Design} 

			The classes of $auroSim$ are classified into one of two groups, classes that represent mechanism dependent on time and classes that are not directly involved in the simulation.
			Objects in the  simulation have causality, and are instantiated from classes derived from the abstract class \emph{timeInterface}.
			These classes inherit the pure virtual functions of \emph{timeInterface}, and are abstract unless the functions are overloaded in the derived classes.
			This assures that all objects directly involved in the simulation have defined its own \emph{doTask()} and \emph{doCalculation()} functions.
			The reader is referred to \cite[chap. 12]{Stroustrup2000} for more about abstract classes in C++, and appendix \ref{appendixUMLofAllNodeSubelementClasses} for $UML$ class diagrams of $auroSim$.
% 			It is referred to \cite[chap. 12]{Stroustrup2000} for more about abstract classes in C++, and appendix \ref{appendixUMLofAllNodeSubelementClasses} for $UML$ class diagrams of $auroSim$.
% 			It is referred to \cite{Stroustrup2000KAP12} for more about abstract classes in C++, and appendix \ref{appendixUMLofAllNodeSubelementClasses} for $UML$ class diagrams of $auroSim$.



			\begin{figure}[htbp!]
				\centering
				\includegraphics[width=0.99\textwidth]{UML/simulatorKlassedesign}
				\caption[UML class diagram of \emph{auroSim}, the neuron simulator designed to compare $NIM$ and $\kappa M$.]{
						UML class diagram for \emph{auroSim}. % of the simulated classes in the simulator.
						All classes directly involved in the simulation are derived from \emph{class timeInterface}.
						The classes listed on the right hand side of the figure are abstract classes meant to be inherited to neuron subelements of the two simulation 
							models, $NIM$ and $\kappa M$. 
						For a derived class of \emph{timeInterface} to be able to make objects, the pure virtual  functions \emph{doTask()} and \emph{doCalculation()} have to be defined for that class.
						This ensures that all objects involved in the simulation have a defined behaviour in time.
						}
				\label{figUMLclassDiagramOfSimulator}
			\end{figure}

			All common aspects of the two simulation models, $NIM$ and $\kappa M$, are located in the abstract neuron subelements classes 
				\emph{i\_dendrite}, \emph{i\_auron}, \emph{i\_axon} and \emph{i\_synapse}.
			These are derived to the model--specific neuron subelements for the $\kappa M$ and $NIM$ simulator.
			Thus, only the functionality that differ between the two models are implemented separately. %, making differences in the design of the two simulation models more distinct.

			The main loop of the simulation is located in the function \emph{void* taskSchedulerFunction(void* )}.
			While \emph{bContinueExecution} is set, the first element of \emph{pWorkTaskQueue} is popped and its \emph{doTask()} member function is executed.
			New elements are inserted at the end of \emph{pWorkTaskQueue}. % Thus, the task scheduler function is responsible for simulating 
%% fjærna..
% 			Thus, the task scheduler function is responsible for driving causality forward in the simulation.
 			In this way, the task scheduler function is responsible for driving causality forward in the simulation.
			A graceful termination of the simulation is possible by setting \emph{bContinueExecution = false}.

\begin{lstlisting}
void* taskSchedulerFunction(void* )
{
	...
	
    // Simulation's main-loop:
    while( bContinueExecution )
    {
        // Pop first element before execution: Save pointer to the 
		// pointer variable pConsideredElementForThisIteration
        static timeInterface* pConsideredElementForThisIteration;
        pConsideredElementForThisIteration = time_class::pWorkTaskQue.front();

        // Then pop element from pWorkTaskQue:
        time_class::pWorkTaskQue.pop_front();

        // Perform task:
        pConsideredElementForThisIteration->doTask();
    }
    return 0;
}
\end{lstlisting}
			
			In order to simulate time by the serial execution performed by \emph{taskSchedulerFunction(void*)}, \emph{time\_class} has been designed as a time separation object.
			As can be seen in fig. \ref{figUMLclassDiagramOfSimulator}, this is where \emph{pWorkTaskQueue} is located as a \emph{static} member.
			Class \emph{time\_class} also contains the variable that represent $t_n$, \emph{static unsigned long ulTime}, and a \emph{doTask()} function that is responsible for incrementation of this variable.
			Since this class is fundamental for time simulation in $auroSim$, and enables time to be emulated by a single linked list, a whole section is reserved for introducing this class.


		\subsection{Time}
		\label{ssecTime}
			Class \emph{time\_class} contains the elements \emph{pWorkTaskQueue}, \emph{pCalculationTaskQueue} and \emph{ulTime} as static member variables.
			Before the main loop of the simulation starts, \emph{pWorkTaskQueue} is initialized by inserting a single \emph{time\_class} object into the linked list. %queue.
			This is done in the function \emph{initializeWorkTaskQueue()}, marked as a friend function of \emph{time\_class}.
			The \emph{friend} keyword is a way of allowing other elements to access the \emph{private} parts of a class declaration\cite[Appendix C.11]{Stroustrup2000}. %\cite{Stroustrup2000KAP11}. 
			The static flag \emph{bPreviouslyInitialized} is used to prevent reinitialization of \emph{pWorkTaskQueue}. 

\begin{lstlisting}
void initializeWorkTaskQue(){
{
	// Flag to prevent reinitialization
	static bool bPreviouslyInitialized = false;
	if(bPreviouslyInitialized)
		return;

	// Insert pointer to object of time_class, allocated in the free store	 
 	time_class::pWorkTaskQue 	.push_back( new time_class() );

	// Set flag to prevent reinitialization of pWorkTaskQue
	bPreviouslyInitialized = true;
}
\end{lstlisting}
			
% TODO Flytt talla fra "Before task execution" til å gjelde "Action" i figuren TODO
% TODO Teikn med når element B og C blir urført også. Dette er interresant, men kan gjøres mindre i figuren, siden dette ikkje er så viktig. TA MED! TODO
%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
%TODO TODO TODO TODO TODO Denne er litt feil: det er ikkje t_n som skal stå på høgresida, men computation-number N. TODO Skriv begge deler (t_n og N)TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
\begin{figure}[htb!p]
	\centering
	\includegraphics[width=0.95\textwidth]{pWorkTaskQueue}
	\caption[A schematic model of time propagation in \emph{auroSim}]{
			The first sketch of how concurrency is simulated in \emph{auroSim}.
			%A schematic model of how concurrency is simulated in \emph{auroSim}.
			taskSchedulerFunction() pops the first element of pWorkTaskQueue and executes its \emph{doTask()} member function.
			1) Element $T$, representing \emph{timeSeparationObj}, iterates \emph{ulTime} and inserts a \emph{self} pointer at the back of \emph{pWorkTaskQueue}.
			2) Element $A$ generates two new tasks, $A_1$ and $A_2$, before the pointer is removed from \emph{pWorkTaskQueue} by \emph{taskSchedulerFunction()}.
% 			No new tasks are generated by the tasks represented by $B$ and $C$.
 			Element $B$ and $C$ do not generate any new tasks.
			Computation nr. $5$ moves  \emph{timeSeparationObj} to the back of the list, and the situation is similar to \emph{pWorkTaskQueue} after action nr. 1.
% 			At time (5), \emph{timeSeparationObj} is again moved to the back of the list, and we get a situation similar to the list after the action at $t_n=(1)$. 
			}
	\label{figTimePropagationbypWorkTaskQueue}
\end{figure}

	Because the \emph{time\_class} object is allocated in the free store, that object will exist for as long as the implementation runs or is explicitly deallocated. 
	A pointer to this element is legal to insert into \emph{std::list$<$timeInterface*$>$ pWorkTaskQueue} since \emph{time\_class} is derived from \emph{class timeInterface}.
	The \emph{time\_class} object inserted into \emph{pWorkTaskQueue}, referred to as {\bf timeSeparationObj} in the remainder of this text, is responsible for administration of time in $auroSim$. 
	When \emph{timeSeparationObj.doTask()} is called, \emph{ulTime} is incremented after a \emph{self}--pointer is pushed to the back of \emph{pWorkTaskQueue}\cite{FDP_report}.
	In this way, \emph{timeSeparationObj} acts as a time separation object, where the execution of its \emph{doTask()} function is the only way a new time step can be initialized in $auroSim$.

	When an element's task is performed from \emph{taskSchedulerFunction(void*)}, the pointer to that element is removed from \emph{pWorkTaskQueue}. 
	Some elements create other tasks, inserting them at the end of \emph{pWorkTaskQueue}.
	Since \emph{timeSeparationObj} defines the separation of two computational time steps, this element lies after all other tasks in the current time iteration. 
	New tasks are therefore inserted by their order of creation in the subsequent time step (after \emph{timeSeparationObj} in \emph{pWorkTaskQueue}).
	This enables a single linked list to behave like the two alternating lists in fig. \ref{figTimePropagationByAlternatingTaskLists}.
	Time simulation with a single linked list, and the use of \emph{timeSeparationObj} is illustrated in fig. \ref{figTimePropagationbypWorkTaskQueue}.

%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TA VEKK NESTE SECTION: trengs ikkje! TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
% 		\subsubsection{Task Scheduling of a $\kappa M$ Node}
% 			The simulation time scheme enables 
% 			Utilizing the proactive firing time computation in the $\kappa M$ simulation scheme demands a more sophisticated way of scheduling tasks.
% 			The direct simulation of a neuron's signal propagation can be executed by a direct approach where only the current and the next computational time step have to be considered.
% 			The $\kappa M$ utilize a more advanced method based on estimation task times, and demands that the implementation considers these to find the firing time of each node.
% 			
% 			Two alternatives for scheduling tasks have been tested for the simulator.
% 			The first is based on a continuously updated linked list of lists with tasks. %that can be considered a variable array.
% 			When a task is scheduled for execution e.g. in the iteration after the next, the object's pointer is inserted into the second inner list in the outer linked list.
% 			Before each time step, the first element of the outer list is popped and all the tasks of the inner list is inserted into \emph{pWorkTaskQueue}.
% 			This gives a list of lists that gives the relative time of scheduled tasks, where each list contains jobs scheduled for future time iterations.
% 			
% 			An alternative approach is to implement time scheduling by letting the \emph{time\_interface} abstract class have a variable \emph{double dEstimatedTaskTime}.
% 			This element is updated every time the neuron's firing time estimate is updated and checked by \emph{time\_class::doTask()} when time is iterated:
% 				If an element is scheduled for execution during the next computational time step, the pointer to that element is inserted into \emph{pWorkTaskQueue}.
% 			This causes the task to be executed during the correct computational time step, 
% 			%As introduced in section \ref{ssecTime}, this causes the task to be executed during the correct computational time step, 
%  			%This causes the task to be executed during the correct computational time step, 
% 				and the double precision floating point variable \emph{dEstimatedTaskTime} enables an intra--iteration time accuracy for tasks. 
% 				% if \emph{pWorkTaskQueue} is sorted by this variable.
% 
% 			The efficiency of the two methods have been tested by comparing the total run time for a similar experiment set up.
% 			Because the second alternative is simpler to implement and thus simpler to maintain,
% 				and because it was found to have about the same grade of efficiency(about $5\%$ faster for the conducted experiment),
% 				%and have about the same grade of efficiency(about $5\%$ faster for the conducted experiment), 
% 				the second approach is used for time scheduling in $auroSim$. % this implementation.
% 				%the alternative with the \emph{time\_interface::dEstimatedTaskTime} is used for time scheduling in this implementation.
% 			%The second alternative was somewhat more efficient($<5\%$ faster run time) in addition to being simpler to implement and maintain.
% 			%This alternative was therefore chosen.
 			

%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO  
% 	%TODO Flytt neste subsubsection! Passer ikkje så godt inn her, og er nok bra en annen plass TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
% 			\subsubsection{Task Scheduling for Other Tasks}
% 				%The task scheduler utilize a variable from \emph{
% 				As the task scheduler use a member variable from \emph{class time\_interface}, task scheduling can be used for all classes that is part of the simulation.
% 				An important example of this is the \emph{synapse}: % The synaptic transmission for all output synapses of a node can therefore 
% 					When the neuron fires, the auron object of the node can write to all the node's output synapses' \emph{dEstimatedTaskTime} variable.
% 				The time can be written to the present time plus the predefined axonic delay before that synapse's transmission.
% 				In this way, a more efficient axon delay can be simulated with floating point accuracy.



 		\section{The Artificial Neuron} 
			The artificial neuron in $auroSim$ is designed as a simplification of the biological neuron as shown in fig. \ref{figFigurAvNeuronet}.
			Each node contains the most important elements of the neuron with regard to signal propagation, %, as illustrated in the sketch in fig. \ref{figModellAvEnkeltauronet}.
				located in four subelements that represent [synapse, dendrite, soma, axon].
			Each subelement of the artificial neuron has a pointer to the previous and the next element in the signal pathway, enabling a direct simulation of the intracellular communication of the neuron.
			
\begin{figure}[hbt!p]
	\centering
	\includegraphics[width=0.90\textwidth]{UML/klasseDiagramForEnkelauronet}.
	\caption[A sketch of the subelement design of a node in the ANN, enabling the intracellular communication scheme used for signal propagation in the artificial neuron]
				{A diagram of the subelements of the artificial neuron.
				The signal is propagated from the left to the right in the figure.
				Transmissions in a synapse calls the postsynaptic dendrite's \emph{newInputSignal()}.
				When it is time for the neuron to fire(checked by \emph{newInputSignal()} in the $NIM$ version of the dendrite), the pointer to the node's auron element is inserted into \emph{pWorkTaskQueue}.
				Auron's \emph{doTask()} function push its axon pointer to the back of \emph{pWorkTaskQueue}, and the axon's delay is simulated in the same manner.
				}
	\label{figUMLClassDiagramForASingleNeuron}
\end{figure}






		\subsection*{Construction of Node Elements}

			The object design of a node enables the implementer to specify the spatio--temporal resolution in the simulation.
			To achieve a higher resolution, a smaller computational time step can be defined.
			The axonic delay before any particular synapse is defined by the number of serially linked axon elements, scaled by the size of the computational time step.


			Since the subelements of the artificial neuron are designed to be separate entities linked by pointers, special effort has to be made to make each node act as a single object.
% 			As each subelement of the artificial neuron is designed to be separate entities linked by pointers, special effort has to be made to make each node act as a single object.
			In $auroSim$, this is achieved by considering the whole node as a ``metaobject'', where all elements are allowed to access the next and previous subelement's protected parts.
			The \emph{friend} keyword allows another class or function to access the \emph{private} or \emph{protected} elements of the class. %Skrevet før. Dessuten har eg gått vekk fra å cite kapittel: \cite{Stroustrup2000KAP11}.
			In this way, all subelements of a node metaclass are given the same privileges as if it was defined to be a single node class.
			This design opens many opportunities for the uses of $auroSim$, but is also makes the construction of a node non--trivial.

			
			To construct the node metaclass object illustrated in fig. \ref{figUMLClassDiagramForASingleNeuron}, it is most convenient to start with a subelement with only one previous and one subsequent element. %%%%
			The only element that satisfies this constraint is the [auron] subelement.			
			The construction of this element can be represented by the notional constructor \emph{auron::auron()}.
\begin{lstlisting}
auron::auron() : timeInterface("auron"){
	...
	pOutputAxon = new axon(this);
	pInputDendrite = new dendrite(this);
	...
}
\end{lstlisting}
			The classes [auron, axon, dendrite and synapse] do not exist in the implementation, but are used in this section to illustrate how the constructor of the model specific
				\emph{s\_\{element\}} and \emph{K\_\{element\}} are designed.
			Because the implementation always use dereferenced pointers, the \emph{free store} is used for the node subelements.
%			Because the implementation always use dereferenced pointers, the \emph{free store} can be used for the node subelements.
			The \emph{new $<T>$} operator allocates memory for an object of type $<T>$ in the free store, and give the same results as \emph{malloc(size($T$))} for memory allocation in C \cite{Stroustrup2000}. %% -- KAP19... %%%%%
			Utilizing the dynamic memory enables a more precise control of the scope of each element existence, 
				as an element in the free store lasts for the remainder of the run or until explicitly deallocated \cite[Appendix~C.9]{Stroustrup2000}.

			
		\subsection*{Destruction of Node Elements}
			To avoid memory leaks in C, elements constructed in the free store have to be explicitly deallocated.
			In $auroSim$, this is done by the subelement's \emph{destructor}.
 
			Like for the construction, the destruction of a whole node starts at the [$auron$] subelement and spreads to the node's more distal parts.
 			For the [$dendrite$] and [$axon$] element, a \emph{while} loop is used to remove all synaptic connections.
\begin{lstlisting}
/*** Deallocation is common for both models' dendrite, and therefore located in i_dendrite ***/
i_dendrite::~i_dendrite()
{
	// Delete all dereferenced pInputSynapse objects. The synapses are responsible for removing its pointer from the presynaptic and postsynaptic node.
	while( !pInputSynapses.empty() ){
	 	delete (*pInputSynapses.begin() );
	}
}
\end{lstlisting}
			The function \emph{std::list::empty()} returns \emph{false} as long as the list contains elements, and \emph{true} if it is empty.
			The function \empty{std::list::begin()} returns a pointer to the first element of the list.
			The free--store memory used by $X$ is deallocated by the function \emph{delete($X$)}. 
			This also calls the destructor of $X$. %XXX ELLER:
% 			This also calls its destructor.

			If an axon sends a signal to a deallocated synapse, the action is undefined and errors might occur.
			To avoid undefined behaviour, the destructor of a class is responsible for removing all pointers to the destructed object.
			This can be seen in the destructor of $s\_synapse$:
\begin{lstlisting}
/*** Destructor for s_synapse ***/
s_synapse::~s_synapse()
{
	// Remove all [this]-pointers from prenode's pOutSynapses-list:
	for( std::list<s_synapse*>::iterator iter = (pPreNodeAxon->pOutSynapses).begin(); iter != (pPreNodeAxon->pOutSynapses).end() ; iter++ ){
		if( *iter == this ){ 	
			//list::erase() calls the elements destructor, but this does not concern us as the element is a pointer. If the element was the object itself, this would create an infinite recursive destructor loop.
			(pPreNodeAxon->pOutSynapses).erase( iter );
		}
	}

	// Remove all [this]-pointers from postnode's pInputSynapses-list:
	for( std::list<s_synapse*>::iterator iter = pPostNodeDendrite->pInputSynapses.begin(); iter != pPostNodeDendrite->pInputSynapses.end() ; iter++ ){
		if( *iter == this ){ 
			//Erase the postsynaptic node's pointer to this synapse:
			(pPostNodeDendrite->pInputSynapses).erase( iter ); 
		}
	}
	...
}
\end{lstlisting}
		The [$synapse$] destructor iterates over all pre-- and postsynaptic elements' synapse pointers, and removes all pointers to itself.
		This shows why the [$dendrite$] element's destructor can safely delete its synapses without consideration of postsynaptic pointers to the synapse element.
% 		This illustrates how the [dendrite] element's destructor can safely delete its synapse pointes so carelessly.
		The function \emph{erase($X$)} also calls the destructor for element $X$, but since the argument is a pointer in the listed code, 
			the pointer's destructor is called instead of the synapse's destructor.
		In this way, an infinite recursive \emph{synapse::$\sim$synapse()} destructive loop is avoided. 



	\section{Class Hierarchy -- Differentiation by Inheritance}

		All classes that are part of the simulation are derived from class \emph{timeInterface} \cite{FDP_report}.
		As seen in fig. \ref{figUMLClassDiagramForAuronElementForNIMandKM}, the pure virtual functions \emph{doTask()} and \emph{doCalculation()} stay undefined in $i\_auron$.
		This is also valid for the other subelement classes of the node metaclass, causing the $i\_\{element\}$ classes to be abstract ($\{element\} \in [$$dendrite$$, $ $auron$$, $ $axon$$,$ $synapse]$).

\begin{quote}
	A class with one or more pure virtual functions is an abstract class, and no objects of that abstract class can be created \cite{Stroustrup2000}.
\end{quote}

		Figure \ref{figUMLClassDiagramForAuronElementForNIMandKM} shows the class diagram for the $auron$ subelement of a node, where it can be seen that all pure virtual functions are overloaded in \emph{s\_auron} and \emph{K\_auron}.
		These classes can therefore be instantiated and objects can be made.
		The $UML$ class diagram of the other subelements are presented in appendix \ref{appendixUMLofAllNodeSubelementClasses}, showing a similar class hierarchy composition for the other node elements.
 		Because all differences between the two models are implemented separately, the similarities and differences between the two models were emphasized to the author.
% 		The most important elements are presented in the remainder of this chapter. % and will now be presented.


\begin{figure}[htb!p]
	\centering
	\centerline{ %To make the figure lie at the center. Useful for figures that have different size than 1\text width
	\includegraphics[width=0.9\textwidth]{UML/classDiagramForAuronSubclass}}
	\caption[UML class diagram for the auron subelement of a node, $NIM$ and $\kappa M$]{
		UML class diagram of the auron subelement of a node.
		The \emph{i\_auron} element in fig. \ref{figUMLClassDiagramForASingleNeuron} is inherited to the model specific classes \emph{s\_auron} and \emph{K\_auron}.
		%It is worth noting how simple the $NIM$ auron is in comparison with the $\kappa M$ model.
		The auron classes are further derived to the sensor\_auron classes for the two models, introduced in section \ref{appendixSensoryNode}.
		}
	\label{figUMLClassDiagramForAuronElementForNIMandKM}
\end{figure}


	\subsection{$NIM$ -- Design and Implementation}

		A spiking neuron is often simulated by a Numerical Integration Method ($NIM$), where the depolarization is found by numerical integration.
		All depolarizing and hyperpolarizing input in the course of a time step is added to the node's depolarization value.
		Leakage is simulated by subtracting a fraction of the difference between the current depolarization value and the defined resting potential.
		For simplicity, the resting membrane potential is defined to be zero in $auroSim$. 
		The leakage constant is written as $\alpha=1-l_f$, where $l_f$ is the leakage fraction.
		In this way, the computation of leakage can be implemented as a single multiplication.

%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
% TODO TODO TODO Ligningene er for dersom man ikkje har input! Dersom man har input gjelder de øverste ligningene, men dette ble litt rotete. Bruk litt tid på denne, og få det correct OG fint! TODO TODO TODO
\begin{equation}
	\begin{split}
		%v(t_n) 	&= v(t_{n-1})-l_f \cdot \left( v(t_{n-1}) + I_{t_n} \right) 	\\
		%		&= (1-l_f)\cdot \left( v(t_{n-1}) + I_{t_n} \right) 				\\
		%		&= \alpha \cdot \left( v(t_{n-1}) + I_{t_n} \right)
		v(t_n) 	&= v(t_{n-1})-l_f \cdot v(t_{n-1})  	\\
				&= (1-l_f)\cdot v(t_{n-1}) 				\\
				&= \alpha \cdot v(t_{n-1})
	\end{split}
	\label{eqLeakageForLIF}
\end{equation}
		
		Because of the order of magnitude for synaptic input connections in the biological neuron, it is highly likely that a neuron receives synaptic input every time step.
% 		Because of the order of magnitude for synaptic input connections is large in the biological neuron, it is highly likely that a neuron receives synaptic input every time step.
		Leakage is therefore computed every time step in $auroSim$.
		For sparse neural networks or for simulations with very small time steps, it could be more efficient to implement leakage as $v(t_n) = \alpha^x \cdot v(t_{n-x})$,
			since the probability of not getting input every time step is larger.

		\subsubsection{The Nodes' Input}
		In $auroSim$, the [$dendrite$] receives all input to the artificial neuron.
		As introduced in sec. \ref{ssecTheAxonAndActionPotential}, the size of the transmission at any particular synapse is defined by the synaptic weight of that synapse.
		Depending on whether the synapse is an \emph{excitatory} or \emph{inhibitory} synapse, the postsynaptic membrane potential is either increased or decreased. %by the size of the synaptic weight after a synaptic transmission.
		In \emph{auroSim}, this is implemented by letting the synapse send [$(1-2\, \text{bInhibitorySynapse})\cdot \omega_{ij}$] as an argument to the postsynaptic dendrite's \emph{newInputSignal(double)} function.
\begin{lstlisting}
inline void s_synapse::doTask()
{
	// If the synapse is inhibitory, send inhibitory signal(subtract):
	//  (bInhibitorySynapse is a boolean variable that defines whether the synapse is inhibitory or not)
 	pPostNodeDendrite->newInputSignal( (1-2*bInhibitoryEffect)*(FIRING_THRESHOLD * dSynapticWeight) );

	// Write to log:
	synTransmission_logFile <<"\t" <<time_class::getTime() <<"\t"
					<<(1-2*bInhibitoryEffect) * dSynapticWeight
					<<" ;   \t#Synpaptic weight\n" ;
}
\end{lstlisting}
		The postsynaptic dendrite's \emph{newInputSignal(double)} function adds the input to the node's depolarization.
		If this variable goes beyond the firing threshold, an action potential is initialized by pushing the node's first axon pointer to \emph{pWorkTaskQueue}.



		\subsubsection{Action potential in $NIM$}
		Spatio--temporal delay in the axon is simulated by a linked list of [$axon$] objects, each pushing the next element on to \emph{pWorkTaskQueue}. % as one of the actions of its \emph{doTask()}.
		For a greater temporal resolution, smaller computational time steps and a larger number of serially linked axon elements can be utilized.
	%%	
		When one of the axon elements contains a pointer to an output synapse, that synapse's pointer is pushed to \emph{pWorkTaskQueue},
			causing synaptic transmission to happen in the following time step. %subsequent time step.



	\subsection{$\kappa M$ -- Design and Implementation}
		As seen in fig. \ref{figUMLClassDiagramForAuronElementForNIMandKM}, the design of \emph{K\_auron} is more complex than for \emph{s\_auron}.
		This is partially because the node has to keep an overview of the floating point time instance for initiation of new time windows.
		A near--continuous resolution for the initiation of time windows, combined with the ability to compute the exact firing time by equation \ref{eqEstimatedTimeToFiring},
			enables the use of intra--iteration firing time accuracy.
		
		Since synaptic flow is utilized instead of discrete synaptic input transmissions, the activation variable of a $\kappa M$ node is defined 
			to represent the activation level $\kappa$ from equation \ref{eqValueEquation}.
		Every time a new time window is initialized, the initial depolarization $v_0 = v(t_0)$ is updated by computing the new value by eq. \ref{eqValueEquationForEachTimeWindow}.
		By also saving the time of initiation, $t_0$, this equation can be used to update the neuron's depolarization the next time a new time window is initialized. %after the new time window.
		This enables $\kappa M$ to be used to simulate the neuron's depolarization by the algebraic value equation.

		%To get an intuitive understanding of how $\kappa$ can be used as the activation variable, 
% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
		% LAG, og vis de to plotta i fig. 3.4 i FDP_final.pdf

		\subsubsection{The Node's Input}	
		In section \ref{ssecSynapticFlow}, discrete synaptic flow is defined as the number of transmissions, $N_{j, \Delta t}$,  scaled by the synaptic weight for that synapse.
\begin{equation}
		\Delta v_{ij}(\Delta t_n) = N_{j, \Delta t} \cdot \omega_{ij, t_{n-1}} \qquad,\;j\in\mathcal{D}
		\nonumber
\end{equation}
		An appropriate description of $\Delta v_{ij}(\Delta t_n)$  is the \emph{synaptic flow of activation level}, since the flow has a direct influence on the postsynaptic node's activation value $\kappa_i$.
		By the use of synaptic flow $\kappa_{ij}$, defined by the presynaptic neuron's activation level, a data format of higher precision than integers can be used.
		The postsynaptic activation level can be written as

\begin{equation}
	\kappa_{i, t_n} = \sum_j \kappa_{ij, t_n} \qquad, \, j\in \mathcal{D}
	\nonumber
\end{equation}
		where $\mathcal{D}$ is the set of integers representing neurons with a synaptic connection to neuron $i$.
		For a $\kappa M$ implementation, it could be advantageous to consider edge transmissions $\kappa_{ij}^*$ as the \emph{change} in synaptic flow. %
\begin{equation}
	\begin{split}
		\kappa_{ij,t_n}^* 	&= \frac{\mathrm{d}}{\mathrm{d}t} {\kappa}_{ij}(t_n) \quad, \, j\in \mathcal{D} \\
							& = \kappa_{ij,t_{n}} - \kappa_{ij,t_{n-1}} 
	\end{split}
	%\nonumber
\end{equation}
		
		When a subset $\mathcal{M}$ of the presynaptic neurons have an altered synaptic flow, this method gives a slightly more efficient simulation ---
			only the edge transmissions from $\mathcal{M}$ have to be added to the postsynaptic node's activation level.
		This can be written as
\begin{equation}
	\kappa_{i, t_n} = \kappa_{i. t_{n-1}} + \sum_l \kappa_{il, t_n}^* \qquad,\, l\in \mathcal{M} \subseteq \mathcal{D}
\end{equation}

		Because edge transmission as the derivative demands numerical integration, the accumulation of error has to be considered.
		A specialized \emph{timeInterface} derived class, whose \emph{doTask()} recalculate the node's activation level is devised for this purpose.
		An object of this class is included as a member variable of the \emph{K\_auron} class, and gives a periodic recalculation of the node's activation level.
		The recalculation is designed to be dynamic, in such a way that the period to the next recalculation is longer if the deviation from the recalculated activation level level is small.
% 		The recalculation is designed to be dynamic, so that if the node's activation variable have a small deviation from the real activation level, the interval to the next recalculation is longer than if the error is large.
		Documentation for \emph{recalcKappaClass} can be found in appendix \ref{appendixRecalculateKappaClass}.
% 		See to appendix \ref{appendixRecalculateKappaClass} for more about \emph{recalcKappaClass}. %the mechanisms of \emph{recalcKappaClass}.
		



		\subsubsection{Action Potential in $\kappa M$}
		As discussed in section \ref{ssecTheActionPotential}, the use of the algebraic solution when simulating the neuron enables the node's spike times to have a near--continuous time resolution.
		In $auroSim$, this is implemented by letting \emph{time\_class::doTask()} insert an $auron$'s pointer when it is estimated to fire during the next time step.
%		In $auroSim$, this is implemented by letting \emph{time\_class::doTask()} insert the auron's pointer when its firing time is estimated to be during the next time step.
		Since this is done before time is incremented, the element will execute its task during the correct computational time step. %time iteration.
		By sorting the \emph{K\_auron} tasks by the \emph{dEstimatedTaskTime} member variable, neuron firing is scheduled by the estimated task time instead of the tasks' order of creation.
		This could be of importance when multiple processing units are utilized for simulation of a large network of neurons.
%		This can show important when multiple processing units are utilized for simulation of a large network of neurons.

		To simulate spatio--temporal delay in the axon, each output synapse is scheduled after its predefined transmission delay.
		For example, if the axonic delay before a synapse is defined to be $2.15$ and the node fires at time $141.2$, the synapse's task can be scheduled for execution at time $143.35$ by writing this time to the synapse's \emph{dEstimatedTaskTime}.
		Due to the mechanisms described in sec. \ref{ssecTime}, the synapse will execute its task at that time without the need for simulation of axonic propagation delay.
		%%
		This gives a more constant work load for the simulation, something that is very advantageous if spiking neuron simulations are to be used for real--time applications.
	




%% XXX Neste er analysisOfTheTwoModels.tex, som gir neste section..
