
%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
%%
%% 	Gjør som Kikki sier! Ho foreslår at eg avslutter første avsnitt med: i denne oppgaven undersøker eg om det er mulig å simulere neuronet uten å bruke numerisk integrasjon. ELLER noke.. 
%% 		Veldig lett: bare nevne, så vidt, kva som kan foventes. Skaper også motivasjon for sensor, for å lese videre, engasjert. Da slepper han å lese som "bla bla bla..."
%% I tillegg, skrive om heilt på slutten: Får no følelsen av at det har vore gjort, FØR (.. finn sjølv. Skreiv om, no. Men har ikkje sjekka. SJEKK IGJEN!)TODO
%
%

%\section{Introduction}

%	TODO Skriv om årsaken til like sitering: at all uncited material har eg funnet fram til sjølv. Viktig å poengtere dette i intro!\\

% TODO TODO TODO Skriv om årsak til lite sitering!!! TODO TODO TODO


	While the digital computer processes information by algorithms, networks of neurons can be said to process information by pattern recognition \cite{TrevesNeuralNetworks}. 
	Thus, the two computational systems utilize different computational schemes, with different capabilities and limitations.
	%The two computational systems thus utilize different computational schemes, with different capabilities and limitations.
	Each can, however, emulate the computational scheme of the other to accomplish certain tasks.
% 	The two systems can however emulate the computational scheme of the other.
	By classification of input and producing output based on previously learned patterns, biological neural systems are capable of performing algorithmic tasks. 
	Digital systems are likewise able to emulate neural abilities by simulating networks of neurons. % in the computer.
	This is referred to as Artificial Neural Networks(ANNs) and is an example of bionics, technology inspired by nature.


	Neurons propagate information by discrete output transmissions.
	% TODO TODO TODO Her har morten markert, etter neste linje. Trur han meiner "the firing threshold"?
	An action potential is initialized after the depolarization of the neuron, defined as a leaky integral of input, goes beyond the \emph{firing threshold} \cite{PurvesNeuroscience}.
	When the action potential reaches a synapse, a synaptic transmission causes the postsynaptic neuron to be depolarized or hyperpolarized, depending on the synapse. 
 	The size of a transmission does not vary with the magnitude of the neuron's input, 
 		but can to a certain extent be defined by the strength of the synaptic connection alone\cite{PrinciplesOfNeuralScience4edKAP10}.
%TODO Har skrevet inn, men ikkje brukt bibliografi: Kandel kap 22-24 
	It is debated whether neurons propagate information mainly as the action potential frequency (``the firing frequency''), or if the exact timing of spikes also is important\cite{gerstnerKistler2002, PrinciplesOfNeuralScience4edKAP30, PrinciplesOfNeuralScience4edKAP21, TrevesNeuralNetworks}. %Kan også ta med \cite{ Maass97networksofSN } (skriver akkurat om dette, i intro)
	The main branch of ANN technology models information propagation as a floating point number, defined by the neuron's immediate input\cite{NeuralNetworksForAppliedSciencesAndEngineering}.
	These ANNs can be said to simulate the neuron in the frequency domain, where the floating point number represents the firing frequency of the neuron\cite{ FDP_report, NeuralNetworksForAppliedSciencesAndEngineering}.

%\begin{quote}
%	When modelling neurons in the frequency domain, information of spike timing is lost\cite{FDP_report}.
%\end{quote}

	Simulating the neuron in the frequency domain is a major simplification of the system, 
		and all information about timing is lost. %the spike timing is lost.
	Such models can therefore not be used for exact simulations of the neuron or where the relative spike time of neurons is important\cite{ gerstnerKistler2002, NEVR3004OmModellane}. %TODO TODO Finn anna referanse. Dette er lett å finne CITE på! TODO
% TODO TODO TODO TODO HER: Ta vekk alle siteringene til meg sjølv? TODO
	An element of particular importance is synaptic plasticity, seen as the foundations of learning and memory in neuroscience\cite{NeuroscienceExploringTheBrain3ed, PrinciplesOfNeuralScience4edKAP12, NEVR3001synPlast, NEVR3003STDP, NEVR3004OmModellane, FDP_report}.
 	In frequency based ANNs, local learning rules are defined by the presynaptic and postsynaptic neuron's firing frequency, $r_j'$ and $r_i$.
\begin{equation}
	\Delta \omega_{ij} = C r_i r_j' \quad,\qquad
%	\Delta \omega_{ij} = \sum_j C r_i r_j' \quad,\qquad
	\begin{tabular}{l}
 		$r_j'$ 			\tiny{ is the presynaptic neuron' s firing frequency} \\
		$r_i$  			\tiny{ is the receiving neuron's firing frequency}  \\
		$\omega_{ij}$ 	\tiny{ it the synaptic weight between neuron $j$ and $i$} \\ %\tiny{ is the synaptic weight form neuron $j$ to neuron $i$} \\
		$C > 0$ \\
	\end{tabular}
	\label{eqHebbsPostulate}
\end{equation}
	where $\omega_{i,j}$ represents the magnitude of the synaptic connection between neuron $j$ and neuron $i$ \cite{NeuralNetworksForAppliedSciencesAndEngineering}.
 	This is a mathematical interpretation of what is referred to as ``Hebbian learning'' after Donald A. Hebb who first proposed this mechanism.
	\emph{Hebb's postulate} states:
\begin{quote}
	When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased. \cite{Hebb1949Kap4}
\end{quote}

 	Both Hebb's postulate and the mathematical interpretation presented in equation \eqref{eqHebbsPostulate} describes a monotonic increase in synaptic weight and is obviously unstable; %, since 
 		any correlation between the two neurons' firing frequencies makes the connection between them stronger, thus increasing the correlation.
	Numerous attempts have been made to develop stable learning rules in frequency based ANNs, with still increasing complexity(See e.g. \cite{ANN_aTutorial, hebbUstabilt, Riedel1992459}). 


%TODO TODO TODO Sjekk alle referanser i neste avsnitt! Har ikkje sjekka. Bare stjåle fra NEVR3003. TODO TODO TODO
	In 1987, Gustafsson et al. proposed that the increase in synaptic weight after a transmission varies with the postsynaptic neurons's depolarization\cite{Gustafsson03011987}. 
	At about the same time, Levy and Steward found that synaptic transmission could cause \emph{Long--Term Depression}($LTD$), a decrease in synaptic weight, after a single transmission\cite{Levy1983791LTDetterSTDP}. 
	These two findings explain how it is possible with a graded synaptic plasticity ranging from negative to positive weight change after a transmission. 
	The resulting learning rule has later been referred to as \emph{Spike--Timing Dependent Plasticity}(STDP), as the postsynaptic depolarization at the time of transmission often correlates with the relative spike time of the pre-- and postsynaptic neuron\cite{reviewSTDP}.
%xxx	 %\cite{RossumStableHebbVedSTDP}, %TODO Sjekk teksten til artikkl, ikkje bare tittel..
%xxx	\cite{NEVR3003STDP, RossumStableHebbVedSTDP},
	The biological background for STDP is discussed in sec. \ref{ssecTheBiologicalSynapse}.



	ANNs that operate in the frequency domain do not contain any information about timing, and STDP can not be used.
	Possible temporal elements in signal processing are also lost.
	A networks of nodes that emulate neurons in the time domain, by simulating the depolarization of the neuron, propagate information as spikes.
	Such ANNs are therefore referred to as Spiking Artificial Neural Networks (SANNs)\cite{gerstnerKistler2002}.
	

	Despite the listed advantages, SANN is not often used for practical applications. 
	The main reason might be the computational complexity of these simulations and the chaotic nature of neural networks. 
	Small errors in each node can have large effects on how the whole network behaves.
	It is shown in section \ref{ssecAnalysisOfErrorsForTheTwoModels} that the error in simulations that utilize numerical integration becomes large unless the computational time step is very small, 
		or the simulated period is short.
%%%%
	In an attempt to avoid a cumulative error, a simulation model that utilizes the algebraic equation for the neuron's depolarization was developed %.
		and is presented in this report.
	The novel model is compared to the existing neuron simulation scheme, to assess whether the model is an improvement with respect to the total simulation error.
	

	This report is written for an audience with some knowledge from cybernetics and systems theory, as well as those who study the field of computational neuroscience.
	%TODO TODO TODO Det er bare kapittel 3 som "requires some knowledge about C++". Skriv om, litt! TODO TODO TODO
	% ELLER "Parts of this report" TODO Vurder dette!
	Parts of chapter \ref{chDesignAndTheroeticalComparison} requires some knowledge of object oriented programming.
	These parts can be considered as documentation for $auroSim$, implemented in C++ and distributed under GPL\cite{gitRepoCommit} , and may be ommited.
%	These parts can be considered as documentation for $auroSim$, implemented in C++ and distributed under GPL (see \cite{gitRepoCommit}) , and may be ommited.
%%% GAMMEL:
% 	Parts of chapter \ref{chDesignAndTheroeticalComparison} requires some knowledge of object oriented programming, however, these may be omitted.
% 	The sections are included partially as a guide to the simulator software, $auroSim$, implemented in C++ and distributed under GPL (see \cite{gitRepoCommit}).
	A deep insight into the mechanisms of biological neural systems is not required, as the most important aspects for signal processing are introduced in chapter \ref{chBackgroundTheory}.

	Chapter \ref{chDevelopmentOfANovelModel} gives an overview of mathematics and concepts used in the novel spiking neuron simulation model, $\kappa M$.
	The system's value equation is found by solving the differential equation for the \emph{Leaky Integrate--and--Fire}($LIF$) neuron's depolarization. 
	The novel simulation scheme can also be used for simulating other neuron models, by utilizing this model's value equation instead of the $LIF$ model's.

	In chapter \ref{chDesignAndTheroeticalComparison}, the design of software intended for a theoretical comparison of the two spiking neuron simulation schemes is presented.
	The design and implementation is done in such a way that as many elements as possible are common between the two implementations, in order to make principal differences prominent.
%	Related to this, the concept of differentiation by inheritance is presented in this chapter.
	The resulting software is referred to as $auroSim$ in this text.


	Chapter \ref{chExperimentalEfficiencyMeasurement} presents two experiments for $auroSim$.
	One considers an idealized situation, and is intended as a test of the design and error analysis of the two simulation models.
	The second experiment considers a more complex input pattern, defined by a sinusoidal activation level.
	This can for example represent an applied stimuli through an electrode or \emph{Local Field Potential Oscillations} of cortical neurons.
	The results from the experiment is used in an efficiency comparison of the two models. %, presented in the same chapter.
% 	The experiment is intended to produce results for an efficiency comparison of the two models. %, presented in the same chapter.
% 	The experiment is intended for the efficiency comparison, presented in the same chapter.
%	The experiment is intended for an efficiency comparison of the two simulation models. %, presented in the same chapter.


	%While the report ends with the conclusion and recommendations for further work, it is recommended for the interested reader to also
	The report concludes with a discussion of the differences in design, implementation, error mechanisms and efficiency of the two models. %, in chapter \ref{chDiscussion}.
	Most chapters end with a discussion of the presented elements, and the final chapter can be seen as a conclusion of all the chapters' summaries.
	The interested reader can find some additional information in the appendix. 
	These elements are excluded from the main text to increase readability.



%RESTEN ER KOMMENTERT UT: Les gjennom det alikevel. Slett det som ikkje er direkte relevant: Gjør dette når du er haudètomm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	The theory for frequency based ANNs is also well established, and can not direcly be used in SANN.
%	FREKVENS-simulering kan ikkje overføres til SPIKE-simulering. Float-variabel vs. diskrete overføringer.



% 	Despite the listed advantages, SANN is seldolmy used for technology\cite{CITE}.
% 	One reason for this is the computational complexity of SANN simulations.
% 	Numerical integration involve accumulation of error, something that might be devastating for spiking neuron simulations.
% 	It is shown that the temporal accuracy(inverse of size of computational time step) is proportional to the simulation error.
% 	In an attempt to make simulations without having to let the simulation's time step be very small or the simulation time be very short, 
% 		a novel neural simulation model have been developed in this work.
 
	





% 	This have made is somewhat hard to 
% 	This report is written for an audience with a good knowledge about programming in C++, some mathematical knowledge and preferably a knowledge about how neurons work.
% 	An extensive overview of how the neurons work is not necessary, as the most important elements are introduced in chapter \ref{chBackgroundTheory}.
% 	As this report is interdisciplinary between two interdisciplinary disciplines, neuroscience and cybernetics, it is hard to know when something should be properly introduced and when this is unnecessary.
% 	Situations will therefore occur where the reader feels that the text is obvious.
% 	These elements are still included to enable a larger audience.
% 	The reader is asked to skim such sections.
% 	%The reader is therefore asked to skim sections that seem obvious.





% 	In the first chapter, the most important background theory is reviewed for readers with little knowledge of neural systems and artificial neural networks.
% 
% 	In chapter \ref{chDevelopmentOfANovelModel},
% 		equations for the neuron's depolarization is presented and elements important for the use of the algebraic equations in a simulator is devised.
% 	%After the fundamentals have been introduced, equations for the neuron's depolarization is presented and elements important to use the algebraic equations in a simulator is devised.
% 	The depolarization equation is found by solving the differential equations for the $LIF$ neuron, as this is the model to be implemented in the simulator based on numerical integration.
% 	Other models can be utilized, but finding these models' depolarization equations is outside the scope of this project.
% 
% 	In chapter \ref{chDesignAndTheroeticalComparison}, the design of a software intended for comparing the two spiking neuron simulation models is presented.
% 	It is important that the two implementations share as much code as possible, as this makes the differences in design and implementations prominent.
% 	The concept of differentiation by inheritance is presented in this chapter.
% 	
% 	Chapter \ref{chExperimentalEfficiencyMeasurement} presents two experiments intended to test the novel simulation model and compare the two models. 
% 	One of the experiments is an idealized situation, and only intended to test the implementation and error analysis of the two models.
% 	The second experiment is a bit more complex, and simulates a sinusoidal activation level that can represent an applied stimuli on a sensory neuron or Local Field Potential Oscillations of cortical neurons.
% 	This experiment is intended as an experimental efficiency comparison of the two simulation models, where efficiency is measured by the temporal resolution needed in a simulation to accomplish some accuracy goal.
% 	%The second experiment is utilized as a test of the efficiency of the two simulation models, as efficiency can be measured by the temporal resolution needed in a simulation to accomplish some accuracy goal.
% 	%The second experiment can be a good way to test the efficiency of the two simulators, as efficiency can be measured by the temporal resolution needed for a simulation to accomplish some accuracy goal.
% 
% 	Finally, the differences in design, implementation, error and efficiency is discussed in chapter \ref{chDiscussion}.
% 	Most chapters end with a discussion about the elements discovered in the chapter, and this final chapter can be seen as a summary of these discussions.
% 	The conclusion and recommendations for further work are also located in this part of the report.









%	It is shown that the speed at which biological neural systems compute certain input, 
%		can not be achieved unless the spike timing is a part of the neural computation. %XXX Cite
%	SKRIVE OM InnØret og VentroLateralNuclei?
%%
%	Networks of nodes that simulates signal propagation by spikes have been referred to as Spiking Artificial Neural Networks(SANN). %XXX Cite
%	SANN have been used for simulations of neural systems[CITER] as well as for technology[CITE]. %TODO TODO
%%	It is used for technology both to approach the fast computation of biological neural systems as well as its learning capabilities.

%Skriv kva gruppe eg skriv til. Kva bakgrunn ser eg for meg at dei har. Eg har tatt med litt bakgrunnsinformasjon om nevro dersom leser ikkje har utpregende kjennskap til dette området. Eg har også forsøkt å ta med litt meir i avsnittene som omhandler C++, ettersom leser også kan være fra neuroscience minjøer uten utpreget kjennskap til programmering. Tilfeller vil difor oppstå der leser har god kunnskap til området, og i dette tilfellet bes leser å skim these sections.


%	Nevn LIFE ---  sjå ssayNEVR3002\_proprieception 
%
%	Simulering av SN: feilaktig simuleing: kvifor der det drit å få feil?\\
%	
%	Vanskelig å unngå akumulativ feil!\\


%	Elements like robustness, ability to handle fuzzy information, fault and failure tolerance, and learning makes ANNs the best tool for handling certain input\cite{jainEtAl}.

% Mi avgrensning.  --Kva har eg utelatt. Kvifor. osv..


%  Ting som må være med :
% 		A statement med målet til teksten. Kvifor vart prosjektet gjort?
% 		Nok bakgrunnsinfo for å forstå kvifor det er viktig å lese vidare.
% 		Proper accnowledgement of previous work on which I am building. Nok referanser til at leser kan gå til biblioteket og finne støttelitteratur før han leser vidare.
% 		The introduction should be focused on the thesis question(s).  All cited work should be directly relevent to the goals of the thesis.  
% 		Scope of project: Kva er med, og kva er ikkje!
% 		Verbal table of context. Vær sikker på at det er veldig klart kva som er bakgrunnsinfo og kva som er mitt arbeid.


%XXX RAPPORT DISPOSISJON:
% Motivasjon: Kvifor er dette gjort?
% Problemområde: Kva er gjort?
% Kva har andre gjort? 		(skive om SANN)
% Mi avgrensning.  --Kva har eg utelatt. Kvifor. osv..
% Disposisjon: Utvida innholdsfortegnelse.

	%Skriv kva gruppe eg skriv til. Kva bakgrunn ser eg for meg at dei har. Eg har tatt med litt bakgrunnsinformasjon om nevro dersom leser ikkje har utpregende kjennskap til dette området. Eg har også forsøkt å ta med litt meir i avsnittene som omhandler C++, ettersom leser også kan være fra neuroscience minjøer uten utpreget kjennskap til programmering. Tilfeller vil difor oppstå der leser har god kunnskap til området, og i dette tilfellet bes leser å skim these sections.
	% Skriv: Eg har plassert eit kapittel i appendix om synaptisk overføring i appendix. Dette er for å få en meir helhetlig oversikt over det biologiske systemet, men siden det ikkje er direkte knyttet til oppgaven(dårlig formulert) er det ikkje med som en del av hovedteksten. Dette kan brukes for utfyllende informasjon og motivasjon for SANN.









% // vim:fdm=marker:fmr=//{,//}
